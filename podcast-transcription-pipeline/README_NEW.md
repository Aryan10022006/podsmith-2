# ğŸ™ï¸ Podcast Transcription and Analysis Pipeline

A sophisticated, real-time-capable podcast transcription and analysis pipeline that extracts rich semantic meaning from audio, diarizes speakers, detects emotions, and stores structured data in a modular, RAG-ready format.

## ğŸ¯ Features

### Core Capabilities
- **High-Accuracy Transcription**: Whisper-based ASR with optimized parameters
- **Speaker Diarization**: Automatic speaker identification and segmentation
- **Emotion Detection**: Text and audio-based emotion analysis
- **Semantic Segmentation**: Intelligent paragraph/topic-based content blocks
- **Multi-Level Summarization**: Block-level and global content summaries
- **Keyword Extraction**: Advanced keyword and keyphrase extraction with trend analysis
- **Comprehensive Validation**: Data quality checks and processing reports

### Technical Features
- **Crash Recovery**: Resumable processing with state persistence
- **Device Optimization**: Automatic GPU/CPU selection with memory management
- **Modular Architecture**: Easy to extend and customize
- **RAG-Ready Output**: Structured data optimized for retrieval-augmented generation
- **Comprehensive Logging**: Detailed processing logs and error tracking

## ğŸ—ï¸ Architecture

```
podcast-transcription-pipeline/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ core/                    # Core pipeline components
â”‚   â”‚   â”œâ”€â”€ session_manager.py   # Session management & recovery
â”‚   â”‚   â”œâ”€â”€ pipeline_orchestrator.py  # Main orchestrator
â”‚   â”‚   â””â”€â”€ audio_processor.py   # Audio preprocessing
â”‚   â”œâ”€â”€ transcription/           # Transcription components
â”‚   â”‚   â”œâ”€â”€ whisper_transcriber.py  # Whisper integration
â”‚   â”‚   â”œâ”€â”€ diarization.py       # Speaker diarization
â”‚   â”‚   â””â”€â”€ transcript_formatter.py  # Output formatting
â”‚   â”œâ”€â”€ analysis/                # Analysis components
â”‚   â”‚   â”œâ”€â”€ emotion_detector.py  # Emotion detection
â”‚   â”‚   â”œâ”€â”€ semantic_segmenter.py  # Content segmentation
â”‚   â”‚   â”œâ”€â”€ topic_classifier.py  # Topic classification
â”‚   â”‚   â”œâ”€â”€ summarizer.py        # Content summarization
â”‚   â”‚   â””â”€â”€ keyword_extractor.py # Keyword extraction
â”‚   â”œâ”€â”€ utils/                   # Utilities
â”‚   â”‚   â”œâ”€â”€ device_manager.py    # Device optimization
â”‚   â”‚   â”œâ”€â”€ validator.py         # Data validation
â”‚   â”‚   â”œâ”€â”€ logger.py           # Logging utilities
â”‚   â”‚   â””â”€â”€ file_handler.py     # File operations
â”‚   â””â”€â”€ config/                  # Configuration
â”‚       â”œâ”€â”€ settings.py         # Settings management
â”‚       â””â”€â”€ model_config.py     # Model configurations
â”œâ”€â”€ output/                     # Output directory
â”‚   â””â”€â”€ sessions/              # Session-based outputs
â”œâ”€â”€ tests/                     # Test suite
â”œâ”€â”€ config.yaml               # Main configuration
â”œâ”€â”€ requirements.txt          # Dependencies
â””â”€â”€ main.py                  # Entry point
```

## ğŸš€ Quick Start

```bash
# 1. Install dependencies
pip install -r requirements.txt

# 2. Process your first podcast
python main.py sample_podcast.mp3

# 3. Check results
ls output/sessions/session_*/
```

## ğŸ“Š Output Structure

For each processed audio file, the pipeline creates a session directory:

```
output/sessions/session_20250712_143022_abc12345/
â”œâ”€â”€ transcript.json           # Transcription with speaker info
â”œâ”€â”€ emotions_text.json        # Text-based emotion analysis
â”œâ”€â”€ emotions_audio.json       # Audio-based emotion analysis
â”œâ”€â”€ semantic_blocks.json      # Semantic segmentation & topics
â”œâ”€â”€ summaries.json           # Block & global summaries
â”œâ”€â”€ keywords_topics.json     # Keywords & trend analysis
â”œâ”€â”€ validation_report.json   # Processing validation
â”œâ”€â”€ processing_log.txt       # Detailed processing log
â””â”€â”€ session_info.json       # Session metadata
```

### Sample Output Files

#### transcript.json
```json
[
  {
    "id": 0,
    "speaker": "Speaker 1",
    "start": 12.4,
    "end": 16.7,
    "text": "Welcome to the show...",
    "confidence": 0.89,
    "words": [...]
  }
]
```

#### semantic_blocks.json
```json
[
  {
    "block_id": 1,
    "text": "...",
    "start": 45.3,
    "end": 91.6,
    "topics": ["AI Ethics", "Bias"],
    "topic_probabilities": {
      "AI Ethics": 0.72,
      "Bias": 0.58
    },
    "speaker": "Speaker 2"
  }
]
```

## âš™ï¸ Configuration

### config.yaml Structure
```yaml
transcription:
  model: "large-v3"
  language: "auto"
  temperature: 0.0

diarization:
  model: "pyannote/speaker-diarization-3.1"
  min_speakers: 1
  max_speakers: 10

emotion:
  text_model: "j-hartmann/emotion-english-distilroberta-base"
  confidence_threshold: 0.5

# ... additional configuration
```

## ğŸ“‹ Requirements

### System Requirements
- Python 3.8+
- 8GB+ RAM (16GB+ recommended)
- 10GB+ free disk space
- GPU with 4GB+ VRAM (optional but recommended)

### Key Dependencies
- PyTorch 2.0+
- OpenAI Whisper
- Transformers (HuggingFace)
- pyannote.audio
- scikit-learn
- sentence-transformers
- librosa

## ğŸµ Usage

### Basic Usage
```bash
# Process a single audio file
python main.py path/to/your/podcast.mp3

# With custom configuration
python main.py path/to/your/podcast.mp3 --config custom_config.yaml

# Resume a specific session
python main.py path/to/your/podcast.mp3 --session-id abc12345
```

### Session Management
```bash
# List all sessions
python main.py --list-sessions

# Check session status
python main.py --status session_20250712_143022_abc12345
```

### Programmatic Usage
```python
from src.core.pipeline_orchestrator import PipelineOrchestrator

# Initialize pipeline
orchestrator = PipelineOrchestrator("config.yaml")

# Process audio
result = orchestrator.process_audio("path/to/audio.mp3")

# Access results
session_dir = result["session_info"]["session_dir"]
validation = result["validation_report"]
```

## ğŸ”§ Advanced Features

### Custom Model Integration
```python
# Use custom emotion detection model
config = {
    "emotion": {
        "text_model": "your-custom/emotion-model",
        "confidence_threshold": 0.7
    }
}
orchestrator = PipelineOrchestrator(config)
```

### Integration with RAG Systems
```python
# Load processed data for RAG
import json
from pathlib import Path

session_dir = Path("output/sessions/session_20250712_143022_abc12345")

# Load semantic blocks for vector database
with open(session_dir / "semantic_blocks.json") as f:
    blocks = json.load(f)
    
# Each block is ready for embedding and storage
for block in blocks:
    text = block["text"]
    metadata = {
        "start": block["start"],
        "end": block["end"],
        "topics": block["topics"],
        "speaker": block["speaker"]
    }
    # Add to your vector database
```

## ğŸ“ˆ Performance Optimization

### GPU Acceleration
- Automatic CUDA detection and utilization
- Memory-optimized batch processing
- Mixed precision support for compatible models

### Processing Tips
1. **Audio Quality**: Higher quality audio = better results
2. **File Length**: 5-60 minute files work best
3. **Speaker Count**: 1-5 speakers optimal for diarization
4. **Hardware**: GPU with 6GB+ VRAM recommended

## ğŸ” Troubleshooting

### Common Issues

**Out of Memory**
```yaml
# Reduce batch size in config
processing:
  batch_size: 2  # Reduce from default 8
```

**Model Download Failures**
```bash
# Clear cache and retry
rm -rf ~/.cache/huggingface/
python main.py your_audio.mp3
```

**Poor Diarization Results**
```yaml
# Try adjusting speaker count
diarization:
  min_speakers: 2
  max_speakers: 5
```

## ğŸ§ª Testing

```bash
# Run all tests
pytest

# Run with coverage
pytest --cov=src tests/

# Run specific test category
pytest tests/unit/
pytest tests/integration/
```

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit changes (`git commit -m 'Add amazing feature'`)
4. Push to branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- OpenAI for Whisper ASR
- Pyannote team for speaker diarization
- HuggingFace for transformer models
- The open-source AI community

---

**Ready to transform your podcasts into structured, searchable, and analyzable data!** ğŸ‰
